<html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Vanik Karslian - dbt</title><meta name=description content="Personal homepage"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet async href=/fontawesome-free-5.12.1-web/css/all.css><script src=/self/js/lazysizes.min.js async></script><link rel=apple-touch-icon sizes=180x180 href="/self/img/icons/favicon/apple-touch-icon.png?v=2"><link rel=icon type=image/png sizes=32x32 href="/self/img/icons/favicon/favicon-32x32.png?v=2"><link rel=icon type=image/png sizes=16x16 href="/self/img/icons/favicon/favicon-16x16.png?v=2"><link rel=manifest href="/self/img/icons/favicon/site.webmanifest?v=2"><link rel=stylesheet href=https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css integrity=sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.4.1.slim.min.js integrity=sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js integrity=sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js integrity=sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6 crossorigin=anonymous></script><link href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Montserrat:400,700" rel=stylesheet><link rel=stylesheet async href=/self/css/custom.css><script>var remark_config={host:"",site_id:'',components:['embed'],max_shown_comments:20,theme:'light',locale:'EN',show_email_subscription:false};(function(c){for(var i=0;i<c.length;i++){var d=document,s=d.createElement('script');s.src=remark_config.host+'/web/'+c[i]+'.js';s.defer=true;(d.head||d.body).appendChild(s);}})(remark_config.components||['embed']);</script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-73265627-3','auto');ga('send','pageview');}</script><script src=https://code.jquery.com/jquery-1.12.4.min.js integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin=anonymous></script><script src=/self/js/load-photoswipe.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css integrity="sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css integrity="sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js integrity="sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js integrity="sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=" crossorigin=anonymous></script><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div></head><body><nav class="navbar navbar-expand-sm navbar-light font-weight-bold border-bottom"><div class=navbar-header><a class=navbar-brand href=http://www.karslian.com/en>Vanik Karslian</a></div><button class=navbar-toggler type=button data-toggle=collapse data-target=#collapsibleNavbar>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=collapsibleNavbar><ul class="navbar-nav ml-auto"><li class="nav-item active"><a class="nav-link navbar-dark" href=http://www.karslian.com/articles/>Articles</a></li><li class="nav-item active"><a class="nav-link navbar-dark" href=http://www.karslian.com/podcasts/>Podcasts</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" id=dropdownMenuButton role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><i class="fas fa-globe"></i>EN</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=dropdownMenuButton><a class=dropdown-item><i class="fas fa-flag-en"></i><b>English</b></a></div></li></ul></div></nav><div class="jumbotron header-font border"><div class=text-center><a href=http://www.karslian.com/en><img class="lazyload rounded-circle rounded-50 border-white bg-white mx-auto avatar-100" data-src=http://www.karslian.com/self/img/avatar_.jpg></a>
<a href=http://www.karslian.com/en><h1 class="text-dark font-weight-bold">Vanik Karslian</h1></a><h2>Seniour Data engineer, DevOps.<br>curious</h2></div></div>Combining dbt and SQLFluff<p>In this post I&rsquo;d like to share how to adopt two very useful tools <a href=https://www.sqlfluff.com/>SQLFluff</a>, a SQL linter,
and <a href=https://www.getdbt.com/>dbt</a>, a data modelling tool, into a powerful combination and make it work with pre-commit to ensure all your
SQLs are properly formatted.</p><p>Why to combine them at all?
There are a lot of ways to write SQL: leading/trailing commas, lower-upper case of keywords,
aliasing table and column names, etc.</p><p>Since every data engineer and analyst spends a lot of time reading SQL code,
having a standard style helps to improve this process and also makes writing less error-prone.</p><p>dbt is a powerful data modelling tool helping to write less boilerplate code with Jinja templates and
it has a remarkable ecosystem of different plugins.</p><p>Let&rsquo;s have a look at the example, say we have the following SQL:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=color:#66d9ef>SELECT</span>
  order_id,
  ORDER_DATE <span style=color:#66d9ef>AS</span> order_date
  , order_price <span style=color:#f92672>*</span><span style=color:#ae81ff>0</span>.<span style=color:#ae81ff>9</span> <span style=color:#66d9ef>AS</span> discounted_price
<span style=color:#66d9ef>from</span> orders
</code></pre></div><p>Doesn&rsquo;t look very reader-friendly, does it?<br>Now I&rsquo;m going to install sqlfluff with pip and create a configuration file for it.<br>In this example I&rsquo;ll be using BigQuery as a query engine, so you might need to adjust my code for your setup,
as well as to enforce certain rules (<a href=https://docs.sqlfluff.com/en/stable/rules.html>reference</a>):</p><p>Installing sqlfluff, dbt and dependent packages:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>pip install sqlfluff
pip install dbt-core
pip install dbt-bigquery
pip install sqlfluff-templater-dbt
</code></pre></div><p>How my <code>sqlfliff</code> looks like:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=color:#66d9ef>[sqlfluff]</span>
<span style=color:#a6e22e>verbose</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>1</span>
<span style=color:#a6e22e>dialect</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>bigquery</span>
<span style=color:#a6e22e>templater</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>dbt</span>
<span style=color:#a6e22e>recurse</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>0</span>
<span style=color:#a6e22e>runaway_limit</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>10</span>
<span style=color:#a6e22e>ignore_templated_areas</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>True</span>
<span style=color:#a6e22e>encoding</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>autodetect</span>
<span style=color:#a6e22e>processes</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>4</span>

<span style=color:#75715e># NB: This config will only apply in the root folder.</span>
<span style=color:#a6e22e>sql_file_exts</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>.sql
</span><span style=color:#e6db74>    </span>
<span style=color:#66d9ef>[sqlfluff:indentation]</span>
<span style=color:#a6e22e>indented_joins</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>False</span>
<span style=color:#a6e22e>indented_using_on</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>False</span>
<span style=color:#a6e22e>template_blocks_indent</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>True
</span><span style=color:#e6db74>    </span>
<span style=color:#66d9ef>[sqlfluff:templater]</span>
<span style=color:#a6e22e>unwrap_wrapped_queries</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>True
</span><span style=color:#e6db74>    </span>
<span style=color:#66d9ef>[sqlfluff:templater:dbt]</span>
<span style=color:#a6e22e>profiles_dir</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>profiles/</span>
<span style=color:#a6e22e>profile</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>bigquery</span>
<span style=color:#a6e22e>target</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>prod
</span><span style=color:#e6db74>    </span>
<span style=color:#66d9ef>[sqlfluff:templater:jinja]</span>
<span style=color:#a6e22e>apply_dbt_builtins</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>True
</span><span style=color:#e6db74>    </span>
<span style=color:#66d9ef>[sqlfluff:rules]</span>
<span style=color:#a6e22e>tab_space_size</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>4</span>
<span style=color:#a6e22e>max_line_length</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>110</span>
<span style=color:#a6e22e>indent_unit</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>space</span>
<span style=color:#a6e22e>comma_style</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>leading</span>
<span style=color:#a6e22e>allow_scalar</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>True</span>
<span style=color:#a6e22e>single_table_references</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>consistent</span>
<span style=color:#a6e22e>unquoted_identifiers_policy</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>all</span>
</code></pre></div><p>Also we need to ignore certain dbt folders by adding them into <code>.sqlfluffignore</code>:</p><pre><code>target/
dbt_modules/
logs/
</code></pre><p>After running <code>sqlfluff fix</code> we get:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=color:#66d9ef>SELECT</span>
    order_id
    , order_date <span style=color:#66d9ef>AS</span> order_date
    , order_price <span style=color:#f92672>*</span> <span style=color:#ae81ff>0</span>.<span style=color:#ae81ff>9</span> <span style=color:#66d9ef>AS</span> discounted_price
<span style=color:#66d9ef>FROM</span> orders
</code></pre></div><p>Integration with pre-commit hooks works following:</p><ul><li><a href=https://pre-commit.com/>Setup pre-commit</a> for your repo.</li><li>Add the hooks into <code>.pre-commit-config.yaml</code> as follows:</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>  - <span style=color:#f92672>repo</span>: <span style=color:#ae81ff>https://github.com/sqlfluff/sqlfluff</span>
    <span style=color:#f92672>rev</span>: <span style=color:#ae81ff>1.1.0</span>
    <span style=color:#f92672>hooks</span>:
      - <span style=color:#f92672>id</span>: <span style=color:#ae81ff>sqlfluff-fix</span>
        <span style=color:#f92672>additional_dependencies</span>: [<span style=color:#e6db74>&#39;dbt-core==1.1.0&#39;</span>, <span style=color:#e6db74>&#39;dbt-bigquery==1.1.0&#39;</span>, <span style=color:#e6db74>&#39;sqlfluff-templater-dbt==1.1.0&#39;</span>]
        <span style=color:#f92672>files</span>: |<span style=color:#e6db74>
</span><span style=color:#e6db74>          models/|
</span><span style=color:#e6db74>          tests/</span>          
      - <span style=color:#f92672>id</span>: <span style=color:#ae81ff>sqlfluff-lint</span>
        <span style=color:#f92672>additional_dependencies</span>: [<span style=color:#e6db74>&#39;dbt-core==1.1.0&#39;</span>, <span style=color:#e6db74>&#39;dbt-bigquery==1.1.0&#39;</span>, <span style=color:#e6db74>&#39;sqlfluff-templater-dbt==1.1.0&#39;</span>]
        <span style=color:#f92672>files</span>: |<span style=color:#e6db74>
</span><span style=color:#e6db74>          models/|
</span><span style=color:#e6db74>          tests/</span>          
</code></pre></div><p>Enjoy!<br>Now everytime you want to commit a new change in your dbt models, sqlfluff will try to fix it automatically for you.<br>Additionally you can integrate this check with Github Actions or other CI/CD tools and require <code>sqlfluff lint</code> to pass
before making merging a new code into the main branch.</p>Overview of Business Intelligence solution for Amazon Vendors<h1 id=intro>Intro</h1><p>In this post I would like to talk about the Business Intelligence solution
for Amazon Vendors we build in FACTOR-A/DEPT. I will shed some light on platforms
Amazon provides for the Vendors, and the challenges it brings to the data teams.</p><h2 id=some-terms-before-we-start>Some terms before we start</h2><p><strong>Vendors</strong> are big enterprises that produce and sell their products on Amazon on
a large scale, they are key suppliers for Amazon.</p><p><strong>ASIN</strong> - a unique identifier used insided of Amazon for every product, ie product ID
inside of Amazon.</p><h1 id=platforms>Platforms</h1><h2 id=advertising-platform>Advertising Platform</h2><p>Advertising platform allows the vendors to set up ad campaigns that will be shown
on the Amazon marketplace itself. The ads appear on various spots in
the search result page and product detail page.
Amazon provides the following reports:</p><ul><li>campaigns</li><li>keywords</li><li>search terms</li><li>asin</li></ul><p>In most of the cases the Vendors are interested in campaign and asin reports. The
first one provides a high level overview on the marketing campaigns,
the latter provides more detailed information on the product level.
In this overview for the simplicity we will skip the keywords & search terms
reports although they have high value for the Vendors.</p><h2 id=dsp>DSP</h2><p>The Demand Side Platform (DSP) which was introduced in the early 2020
became an important tool for the Amazon Vendors. It allows to run advertising
campaigns outside of Amazon itself. For instance, in the mobile apps,
arbitrary retailing web-sites. The idea behind is to drive the traffic
from third-party platforms to Amazon and increase the sales within Amazon itself.
The reports provided in DSP:</p><ul><li>Campaign</li><li>Inventory</li><li>Audience</li><li>Product</li><li>Tech And Geo</li></ul><p>Again, for the simplicity reasons in this article we will focus only on the Campaign and Product reports.</p><h2 id=vendor-central>Vendor Central</h2><p>The Vendor central is the main platform for the vendors. It is a place
where the vendors upload their assortment, the content, pictures, bullet points
for products, and last but not least download the retail related reports such
as organic sales reports.</p><p>The reports available:</p><ul><li>Sales</li><li>Inventory Health</li><li>Amazon Search Terms</li><li>Forecasting</li><li>Net Pure Product Margin</li><li>Traffic</li></ul><h1 id=limitations-on-amazon>Limitations on Amazon</h1><p>The data provided by Amazon has some limitations you need to consider when you start
a BI project. From the business side the obstacle you face working on Amazon is
the <em>data availablility</em> and <em>data integration</em></p><h2 id=data-integration>Data integration</h2><p>As it was mentioned earlier amazon offers 3 main platforms for their vendors
(Advertising Platform, DSP, Vendor Central).
So the first challenge would be to pool the data from these 3 platforms. Unfortunetly,
amazon does not have any integration tools for these platforms, they are completely
independant by design, there is no single sign on (SSO) that would simplify the login process.
Not to mention the fact that even within Vendor Central itself the Vendors are forsed
to have independent accounts per country.
Now imagine how much time would it take to build a global overview of advertising, DSP
and retail for a Vendor? In practice, it would mean that you need to:</p><ol><li><p>login to the Advertising platform</p><p>1.1 chose the account you need</p><p>1.2 navigate to the reports</p><p>1.3 download the reports</p></li><li><p>login to the DSP</p><p>2.1 chose the account you need</p><p>2.2 navigate to the reports</p><p>2.3 download the reports</p></li><li><p>login to the Vendor Central</p><p>3.1 navigate to the reports</p><p>3.2 download the reports</p></li></ol><p>and you have to repeat same steps for every country where you sell your products.
Once the reports are collected your teem needs to combine dozens of spreadsheets in
a meaningful way and provide the final report.
Hopefully, there were no mistakes in between, so you can lean your business decisions
on this manual process.
Now imagine, that you have to repeat same steps next week/month, depending
how often you review your numbers.
Obviously, this approach is error-prone, time-consuming and painful.</p><p>At the moment of writing there was no solution on the market that would solve most of
the integration problems on Amazon.</p><h2 id=data-availability>Data availability</h2><p>In all of the 3 platforms Amazon set limits on both the <em>history available</em> to download,
and the <em>level of detail (granularity</em>) of reports.</p><p><img src=/self/img/2022-09-27-bi-solution-for-amazon-vendors-overview/amazon_grane_and_limits.png alt="Drag Racing"></p><p>The limited history in Advertising and DSP requires extra passion when developing
the data consumers, any issue/bug with data-consumers on production might lead to
the gaps in data, which will not be available anymore.</p><h2 id=data-reliability>Data reliability</h2><p>Having more than 4 years experience with data from Amazon we revealed extraordinary number
of cases when Amazon changes the numbers back in the past. The reports from Vendor Central
Are heavily affected by this issue. Although, the minor changes and volatility of
retail data is expected due to the product returns/amazon internal cache issues/data lags
the changes amazon introduces to the historical reports go beyond expected borders.
Particularly, the numbers might change for the reports more than 12 months old with the
order of magnitude of 10-15% in mission-critical metrics.
In practice, it means that the reports you have downloaded last week/month/quarter will not
match with the same reports downloaded today.
Since most of the customers would use the UI of Vendor Central as a source of the truth,
they would expect to see the same numbers in the DWH.
Which makes the <em>incremental updates</em> (last week/month) impossible, because the data might change back in the
history for no reason. However, the full re-downloads are also quite expensive in terms of
the resources and costs, furthermore, they will not save as from the cases when amazon
updates the history older than 12 moths, since it will not be available for re-download through the API.
In our team were able to build a tradeoff between the cots and the data discrepancies.
We have build automation scripts that do check the current values in the Vendor Central UI on regular basis,
the number are then compared with the numbers in DWH and for the months with discrepancies
more than 3% on we trigger re-downloads.</p><h2 id=data-consistency>Data consistency</h2><p>Another criteria, which is highly related with the data reliability is the data consistency.
Back in the past, we revealed various cases when the number in Adverting UI dashboards,
Advertising API and Advertising downloadable reports do not match between each other.
Although, we haven&rsquo;t seen these issues within last couple of years, the risk that amazon
messes up the data is always there.
Unfortunately, we can not fix the data-issues in this case, due to the fact that all
available sources show different numbers.</p><h1 id=dwh-architecture>DWH Architecture</h1><p>Under this section I will provide a bird eye view on the Architecture of our BI product.
On the very high level the architecture is broken on the:</p><ul><li>data collection area</li><li>data processing area</li><li>data representation area
<img src=/self/img/2022-09-27-bi-solution-for-amazon-vendors-overview/bi_architecture_schema.png alt="The architecture"></li></ul><p>The main purpose of the <em>data collection area</em> is to pull the data from Amazon and store
it in our Data Lake (Google Cloud Storage) for the long term storage.
The downloads are established in incremental manner, meaning every day we do download
the recent updates from Amazon.</p><p>The data from the Data Lake gets consumed by the <em>processing area</em>, where the ELT processing
is happening. Here, the data is stored in BigQuery, the transformations are implemented
using <em>dbt</em>, and the final reporting layer is exposed for the representation area.
The goal of the representation area is to make the data available for consumption.
And here we have two options, the first option is to provide regular dumps
of the &ldquo;raw&rdquo; data through Google Cloud Storage buckets. This option is usually
preferable for our customers who already have data teams internally and some
sort of BI tools implemented. Depending on the customer specifics
they might have some basic sources connected to BI, like
Facebook/Instagram/Google Ads reports, internal ERP, CRM systems or any other
source based on the internal processes of the customer. So, the amazon data is the
only missing piece of the puzzle.
The second option we offer to our customers are the BI Dashboards. The dashboards are
highly configurable and can reflect all of the aspects of the cusomer busines in Amazon.
We can build drill down views starting from the top level view on all of the accounts
in all of the regions till the detailed view on a specific product in a specific
country and amazon account.</p><h1 id=summary>Summary</h1><p>Overall, running business on Amazon is highly complicated. Once the sellers make
it through by passing all non-trivial burocratic steps on initialisation phase, they run
into completely another set of ongoing data related problems. Being truly
data-driven is crucial for many of the customers, since the Amazon market is highly
competitive, and you have to be smart when it comes to marketing and sales strategies.
Ideally, all of your decisions should be backed by data
(reliable, fresh, accurate, and trustworthy data). As we covered in the amazon limitation section
amazon lacks of data availability and integration. We in Factor-a try to solve many
of the challenges on Amazon through our Business Intelligence solution equipping our
customers with a powerful tool, that of course will not make their business successfull
right away, but at will definitely help to set their path towards that direction.</p>Implementing slim CI for dbt with GitHub Actions<p>Recently I&rsquo;ve started working with dbt, and have to say it&rsquo;s quite amazing.
With this post I&rsquo;d like to share the approach for deploying only newly created/modified models, known as slim CI.</p><p>Imagine we have an Airflow instance executing our dbt DAG daily.</p><p>In case we introduce structural changes in our models, or simply by creating a new model,
we don&rsquo;t want to wait till Airflow starts on schedule.
Also, we don&rsquo;t want to fire the whole pipeline execution, because it might be costly to run it multiple times per day.</p><p>What we could do in this case is to introduce a new CI step,
which will compare differences between old and new models,
and run only those, which have changed (and their dependencies/downstream tasks).</p><p>How it works with GitHub Actions:</p><p>say we have <code>deploy_dbt.yml</code> file describing our CI actions</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yml data-lang=yml><span style=color:#f92672>name</span>: <span style=color:#e6db74>&#39;Deploy dbt&#39;</span>

<span style=color:#f92672>on</span>:
  <span style=color:#f92672>push</span>:
    <span style=color:#f92672>branches</span>:
      - <span style=color:#ae81ff>main</span>
<span style=color:#f92672>env</span>:
  <span style=color:#f92672>DBT_VERSION</span>: <span style=color:#ae81ff>1.0.0</span>
<span style=color:#f92672>jobs</span>:
  <span style=color:#f92672>dbt-slim-ci</span>:
    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Deploy dbt models</span>
    <span style=color:#f92672>runs-on</span>: <span style=color:#ae81ff>ubuntu-latest</span>
    <span style=color:#f92672>permissions</span>:
      <span style=color:#f92672>actions</span>: <span style=color:#ae81ff>read</span>
      <span style=color:#f92672>contents</span>: <span style=color:#ae81ff>read</span>
    <span style=color:#f92672>steps</span>:
      - <span style=color:#f92672>uses</span>: <span style=color:#ae81ff>actions/checkout@v2</span>
      <span style=color:#75715e># Get commit hash of the last successful deployment, it will be used for comparison</span>
      - <span style=color:#f92672>uses</span>: <span style=color:#ae81ff>nrwl/last-successful-commit-action@v1</span>
        <span style=color:#f92672>id</span>: <span style=color:#ae81ff>last_successful_commit</span>
        <span style=color:#f92672>with</span>:
          <span style=color:#f92672>branch</span>: <span style=color:#e6db74>&#39;main&#39;</span>
          <span style=color:#f92672>workflow_id</span>: <span style=color:#e6db74>&#39;deploy_dbt.yml&#39;</span>
          <span style=color:#f92672>github_token</span>: <span style=color:#ae81ff>${{ secrets.GITHUB_TOKEN }}</span>
      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Checkout last successful main</span>
        <span style=color:#f92672>uses</span>: <span style=color:#ae81ff>actions/checkout@v2</span>
        <span style=color:#f92672>with</span>:
          <span style=color:#f92672>ref</span>: <span style=color:#ae81ff>${{ steps.last_successful_commit.outputs.commit_hash }}</span>
          <span style=color:#f92672>path</span>: <span style=color:#ae81ff>last_successful_main/</span>
      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Install dbt &amp; packages</span>
        <span style=color:#f92672>run</span>: |-<span style=color:#e6db74>
</span><span style=color:#e6db74>          pip3 install dbt==${{ env.DBT_VERSION }}
</span><span style=color:#e6db74>          dbt deps</span>          
      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Generate last successful manifest.json</span>
        <span style=color:#f92672>run</span>: &gt;<span style=color:#e6db74>
</span><span style=color:#e6db74>          dbt compile
</span><span style=color:#e6db74>          --project-dir last_successful_main/
</span><span style=color:#e6db74>          --profiles-dir last_successful_main/</span>          
      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>Build and test new models</span>
        <span style=color:#f92672>run</span>: &gt;<span style=color:#e6db74>
</span><span style=color:#e6db74>          dbt build
</span><span style=color:#e6db74>          --select state:modified+
</span><span style=color:#e6db74>          --defer
</span><span style=color:#e6db74>          --state last_successful_main/target/</span>          
</code></pre></div><p>Happy coding!</p><footer class="footer card-footer text-secondary mt-auto"><div class=row><div class="col col-sm-9">&copy; 2022 Vanik Karslian</div><div class="col-12 col-sm-3 text-sm-right px-3"><a href=mailto:vanik.karslian@gmail.com class="text-nowrap text-secondary"><i class="fas fa-envelope fa-fw valign-middle" data-hint=Email></i></a><a href=https://www.linkedin.com/in/vanik-karslian/ class="text-nowrap text-secondary"><i class="fab fa-linkedin-in fa-fw valign-middle" data-hint=LinkedIn></i></a><a href=https://github.com/Vanik888 class="text-nowrap text-secondary"><i class="fab fa-github fa-fw valign-middle" data-hint=Github></i></a><a href=https://www.instagram.com/vanik_kars class="text-nowrap text-secondary"><i class="fab fa-instagram fa-fw valign-middle" data-hint=Instagram></i></a><a href=https://t.me/vanik_kars class="text-nowrap text-secondary"><i class="fab fa-telegram fa-fw valign-middle" data-hint=Telegram></i></a></div></div></footer></body></html>